# -*- coding: utf-8 -*-
"""git-mod hw2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/13hr3xU5FsTz6FlGLk3diF7CmC7MdYxDr

In this homework, we will solve a classic problem using $k$-NN. The problem is to classify grayscale images of handwritten digits (28 pixels by 28 pixels) into their 10 categories (0 to 9), based on the [MNIST database](http://yann.lecun.com/exdb/mnist/), which consists of 60,000 training images and 10,000 test images. Let's first import the data:
"""

from google.colab import drive
drive.mount('/content/drive')

import numpy as np
import matplotlib.pyplot as plt
from datetime import datetime
from sklearn.neighbors import KNeighborsClassifier
from torchvision import datasets
train_data = datasets.MNIST(root='/content/drive/MyDrive/data', train=True, transform=np.array)
test_data = datasets.MNIST(root='/content/drive/MyDrive/data', train=False, transform=np.array)
train_X = np.array([train_data[i][0] for i in range(len(train_data))])
train_y = np.array([train_data[i][1] for i in range(len(train_data))])
test_X = np.array([test_data[i][0] for i in range(len(test_data))])
test_y = np.array([test_data[i][1] for i in range(len(test_data))])

"""We can plot the first 10 images with their labels:"""

fig, ax = plt.subplots(1, 10, figsize=(17, 2))
for i in range(10):
    ax[i].imshow(train_X[i], cmap='gray')
    ax[i].set_title('label ' + str(train_y[i]))
    ax[i].axis('off')

nsamples, nx, ny = train_X.shape
# reshaping the data for KNeighborsClassifier
train_X = train_X.reshape((nsamples,nx*ny))
test_X = test_X.reshape((len(test_X),nx*ny))
print('train_X:'+str(train_X.shape)+', train_y:'+str(train_y.shape)+', test_X:'+str(test_X.shape)+', test_y:'+str(test_y.shape))

"""Please keep the following for timekeeping:"""

start_time = datetime.now()

"""Now use `KNeighborsClassifier` from [scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html) with `n_neighbors=3` to generate your predictions on the 10,000 test images, and calculate your accuracy (if you correctly classify 9,510 images, your accuracy is 0.951). You only need to report your accuracy, not your 10,000 predictions:"""

from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

knn = KNeighborsClassifier(n_neighbors=3)
knn.fit(train_X, train_y) 
predicted = knn.predict(test_X)
print("Accuracy Score:")
print(accuracy_score(test_y, predicted))

"""Moreover, plot the first 30 images (in 3 rows and 10 columns) you didn't classify correctly, and display the correct label and your incorrect prediction in the title of each image (`label y pred z` where `y` is the correct label and `z` is your incorrect prediction)."""

# reshaping test data to original form to plot images
test_X = test_X.reshape((len(test_X),nx,ny))    
fig, ax = plt.subplots(3, 10, figsize=(25, 5))

# counter variable to used to check that only 30 incorrectly predicted images are being printed
counter=0
index=0
# we use flag to ensure that once a value is found that matches the if condition, then loop starts from the next element, and not from the starting again
flag = 0
for j in range(10):
  for i in range(3):
    for index in range(flag,10000):
      if (test_y[index]!=predicted[index] and counter<30) :
        flag=index+1
        ax[i][j].imshow(test_X[index], cmap='gray')
        ax[i][j].set_title('label' + str(test_y[index]) + 'pred' + str(predicted[index]))
        ax[i][j].axis('off')
        counter+=1
        break;